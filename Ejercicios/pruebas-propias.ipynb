{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector para 'documento': [ 7.6966463e-03  9.1206422e-03  1.1355019e-03 -8.3250795e-03\n",
      "  8.4250160e-03 -3.6962307e-03  5.7421732e-03  4.3915794e-03\n",
      "  9.6899448e-03 -9.2934975e-03  9.2084054e-03 -9.2815282e-03\n",
      " -6.9077122e-03 -9.1021946e-03 -5.5471100e-03  7.3688962e-03\n",
      "  9.1644777e-03 -3.3253515e-03  3.7230505e-03 -3.6252034e-03\n",
      "  7.8814710e-03  5.8668759e-03  2.0861626e-07 -3.6286747e-03\n",
      " -7.2243060e-03  4.7686161e-03  1.4529788e-03 -2.6131857e-03\n",
      "  7.8378068e-03 -4.0496145e-03 -9.1489861e-03 -2.2554707e-03\n",
      "  1.2514711e-04 -6.6392552e-03 -5.4866159e-03 -8.4997769e-03\n",
      "  9.2298733e-03  7.4240281e-03 -2.9524326e-04  7.3676636e-03\n",
      "  7.9507884e-03 -7.8357337e-04  6.6120909e-03  3.7675237e-03\n",
      "  5.0768424e-03  7.2529912e-03 -4.7393893e-03 -2.1855331e-03\n",
      "  8.7312341e-04  4.2362059e-03  3.3043313e-03  5.0958274e-03\n",
      "  4.5864857e-03 -8.4385090e-03 -3.1838394e-03 -7.2367596e-03\n",
      "  9.6814223e-03  5.0065992e-03  1.7084122e-04  4.1129780e-03\n",
      " -7.6561309e-03 -6.2946510e-03  3.0763936e-03  6.5346383e-03\n",
      "  3.9498745e-03  6.0180221e-03 -1.9861318e-03 -3.3451295e-03\n",
      "  2.0717025e-04 -3.1943608e-03 -5.5169044e-03 -7.7885604e-03\n",
      "  6.5355431e-03 -1.0903371e-03 -1.8908798e-03 -7.8047751e-03\n",
      "  9.3375733e-03  8.6814165e-04  1.7696369e-03  2.4916660e-03\n",
      " -7.3859929e-03  1.6388226e-03  2.9765631e-03 -8.5670296e-03\n",
      "  4.9558021e-03  2.4334085e-03  7.4979127e-03  5.0442982e-03\n",
      " -3.0317164e-03 -7.1629370e-03  7.0962133e-03  1.9015349e-03\n",
      "  5.1992359e-03  6.3811089e-03  1.9122792e-03 -6.1276113e-03\n",
      " -6.2966346e-06  8.2682976e-03 -6.0985480e-03  9.4382809e-03]\n",
      "Vector para 'Este': [-8.2426779e-03  9.2993546e-03 -1.9766092e-04 -1.9672764e-03\n",
      "  4.6036304e-03 -4.0953159e-03  2.7431143e-03  6.9399667e-03\n",
      "  6.0654259e-03 -7.5107943e-03  9.3823504e-03  4.6718083e-03\n",
      "  3.9661205e-03 -6.2435055e-03  8.4599797e-03 -2.1501649e-03\n",
      "  8.8251876e-03 -5.3620026e-03 -8.1294188e-03  6.8245591e-03\n",
      "  1.6711927e-03 -2.1985089e-03  9.5136007e-03  9.4938548e-03\n",
      " -9.7740470e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
      "  2.0227872e-03  4.3050171e-04  6.7363144e-04 -3.8206363e-03\n",
      " -7.1402504e-03 -2.0888723e-03  3.9238976e-03  8.8186832e-03\n",
      "  9.2591504e-03 -5.9759365e-03 -9.4026709e-03  9.7643770e-03\n",
      "  3.4297847e-03  5.1661171e-03  6.2823449e-03 -2.8042626e-03\n",
      "  7.3227035e-03  2.8302716e-03  2.8710044e-03 -2.3803699e-03\n",
      " -3.1282497e-03 -2.3701417e-03  4.2764368e-03  7.6057913e-05\n",
      " -9.5842788e-03 -9.6655441e-03 -6.1481940e-03 -1.2856961e-04\n",
      "  1.9974159e-03  9.4319675e-03  5.5843508e-03 -4.2906962e-03\n",
      "  2.7831673e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
      "  4.3234206e-03 -5.8143795e-03 -8.0419064e-04  8.1000505e-03\n",
      " -2.3600650e-03 -9.6634552e-03  5.7792603e-03 -3.9298222e-03\n",
      " -1.2228728e-03  9.9805174e-03 -2.2563506e-03 -4.7570644e-03\n",
      " -5.3293873e-03  6.9808899e-03 -5.7088719e-03  2.1136629e-03\n",
      " -5.2556600e-03  6.1207139e-03  4.3573068e-03  2.6063549e-03\n",
      " -1.4910829e-03 -2.7460635e-03  8.9929365e-03  5.2157748e-03\n",
      " -2.1625196e-03 -9.4703101e-03 -7.4260519e-03 -1.0637414e-03\n",
      " -7.9494715e-04 -2.5629092e-03  9.6827205e-03 -4.5852066e-04\n",
      "  5.8737611e-03 -7.4475873e-03 -2.5060738e-03 -5.5498634e-03]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Corpus de ejemplo\n",
    "corpus = [\n",
    "    'Este es el primer documento.',\n",
    "    'Este documento es el segundo documento.',\n",
    "    'Y este es el tercer documento.',\n",
    "    'Espero que te sirva esta respuesta.'\n",
    "]\n",
    "\n",
    "# Paso 1: Crear el vectorizador CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Ajustar y transformar el corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Obtener el vocabulario\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Paso 2: Obtener la lista de palabras\n",
    "word_list = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Paso 3: Crear modelo Word2Vec\n",
    "model = Word2Vec(sentences=[sentence.split() for sentence in corpus], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Ver los vectores de palabras\n",
    "word_vectors = model.wv\n",
    "\n",
    "# Palabras para las que queremos ver los vectores\n",
    "words_to_check = ['documento', 'Este']\n",
    "\n",
    "\n",
    "# Mostrar los vectores de las palabras especificadas\n",
    "for word in words_to_check:\n",
    "    if word in word_vectors:\n",
    "        print(f\"Vector para '{word}': {word_vectors[word]}\")\n",
    "    else:\n",
    "        print(f\"No se encontr√≥ el vector para '{word}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencias en el corpus:\n",
      "['Este', 'es', 'el', 'primer', 'documento.']\n",
      "['Este', 'documento', 'es', 'el', 'segundo', 'documento.']\n",
      "['Y', 'este', 'es', 'el', 'tercer', 'documento.']\n",
      "['Espero', 'que', 'te', 'sirva', 'esta', 'respuesta.']\n",
      "Lista de palabras:\n",
      "['documento' 'el' 'es' 'espero' 'esta' 'este' 'primer' 'que' 'respuesta'\n",
      " 'segundo' 'sirva' 'te' 'tercer']\n"
     ]
    }
   ],
   "source": [
    "# Imprimir sentencias en el corpus\n",
    "print(\"Sentencias en el corpus:\")\n",
    "for sentence in corpus:\n",
    "    print(sentence.split())\n",
    "\n",
    "# Imprimir lista de palabras\n",
    "print(\"Lista de palabras:\")\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['este', 'es', 'el', 'primer', 'documento', '.'], tags=['0']), TaggedDocument(words=['este', 'documento', 'es', 'el', 'segundo', 'documento', '.'], tags=['1']), TaggedDocument(words=['y', 'este', 'es', 'el', 'tercer', 'documento', '.'], tags=['2']), TaggedDocument(words=['espero', 'que', 'te', 'sirva', 'esta', 'respuesta', '.'], tags=['3'])]\n",
      "{'.': 0, 'documento': 1, 'el': 2, 'es': 3, 'este': 4, 'respuesta': 5, 'esta': 6, 'sirva': 7, 'te': 8, 'que': 9, 'espero': 10, 'tercer': 11, 'y': 12, 'segundo': 13, 'primer': 14}\n",
      "Vectores de los documentos:\n",
      "Vector para documento 1: [-0.00750868 -0.00617679 -0.01010021  0.01038671  0.00426433 -0.00102669\n",
      " -0.009585   -0.00389662 -0.00974835  0.00184762  0.00243056  0.0036832\n",
      " -0.00442298 -0.00301255 -0.00169233 -0.00994173  0.00277833  0.00995451\n",
      " -0.0099255  -0.00295051 -0.00493095  0.00326262 -0.00553351  0.00524164\n",
      "  0.00537313 -0.00738456 -0.00996762 -0.01048874  0.00503789 -0.01104722\n",
      "  0.00622715  0.00671467 -0.00603196 -0.00452613 -0.00236525  0.00211282\n",
      " -0.00304058 -0.01022138 -0.00450514  0.00070512 -0.00116792 -0.00607175\n",
      "  0.00440486 -0.00869169  0.00199063 -0.00574195 -0.0003129  -0.00048386\n",
      "  0.00475702 -0.00833512 -0.00392684  0.00038245 -0.007541   -0.00728201\n",
      " -0.00293703  0.00937772 -0.00217501  0.0054347  -0.00737253  0.00909755\n",
      "  0.00437908  0.01234799  0.00594568 -0.00484795  0.00268724 -0.00379239\n",
      "  0.0063043   0.00213902 -0.00349373 -0.0051485  -0.00990266 -0.0010597\n",
      " -0.01027568 -0.00952165 -0.00640657  0.00214592 -0.00589736 -0.00974557\n",
      "  0.00173637  0.00277974  0.0073758   0.00553297 -0.01177038  0.00066025\n",
      "  0.0072084   0.00471382  0.00246992 -0.00534288  0.00632056  0.00096899\n",
      " -0.00987848  0.00754231 -0.01201545 -0.00827494 -0.00472838  0.01228321\n",
      "  0.00304995 -0.00140628  0.01098205  0.00203585]\n",
      "Vector para documento 2: [-7.8849429e-03 -3.5148934e-03 -6.0500521e-03  5.4062982e-03\n",
      " -5.9592575e-03 -3.2663562e-03 -6.8989629e-04  8.3803255e-03\n",
      "  6.6789859e-03  3.4160092e-03 -6.2640826e-04 -5.4604420e-03\n",
      " -2.2185841e-03 -5.4139816e-03  1.0408272e-02 -9.6006542e-03\n",
      " -9.5526539e-03 -1.2279734e-04  3.1637740e-03  8.5705658e-03\n",
      "  7.6685385e-03  6.6792928e-03 -5.9864977e-03  1.1201743e-02\n",
      " -9.9524036e-03  1.3702965e-03 -8.1906822e-03 -1.2305047e-03\n",
      "  2.6269795e-03 -9.4655538e-03  4.2598103e-03  1.1481331e-03\n",
      "  5.2607222e-05  6.0501583e-03 -5.4640500e-03 -1.1371197e-03\n",
      " -5.1947153e-04  1.2869438e-03 -9.6149025e-03 -6.1996328e-03\n",
      "  8.8349450e-03  2.1656626e-03  4.0971716e-03 -9.4289724e-03\n",
      " -2.7360730e-03  1.1547111e-03  1.7379692e-03  1.0548264e-02\n",
      " -4.6035289e-04 -5.2579981e-03 -5.1595080e-03  9.9146245e-03\n",
      "  6.4945337e-03 -9.7471513e-03 -4.6078418e-03  6.1856220e-03\n",
      "  2.1176490e-04  1.0707952e-02 -7.3454636e-03 -2.7761983e-03\n",
      "  9.8804561e-03  8.4999586e-03  1.0380949e-02 -3.7718602e-03\n",
      "  2.6518758e-03 -6.2021236e-03 -5.2452115e-03 -5.3814361e-03\n",
      " -9.3824090e-03 -1.4272899e-03  1.6064856e-03 -1.7396315e-03\n",
      "  1.7545111e-03  4.9618306e-03  8.9838626e-03  2.5322700e-03\n",
      "  8.2582040e-03 -5.8382591e-03 -5.3297929e-03 -6.4081117e-04\n",
      "  4.9279751e-03  5.7270336e-03 -1.0486088e-02 -2.0622616e-03\n",
      " -8.7343156e-03  1.0871951e-02 -2.1843589e-03 -6.7111652e-04\n",
      "  8.4021706e-03  1.2978590e-03 -9.7506000e-03  7.3379884e-03\n",
      "  2.4902455e-03  7.8429244e-03 -3.5682265e-03  6.7799292e-03\n",
      "  4.7836401e-03 -3.6068801e-03  4.2874329e-03  4.8827417e-03]\n",
      "Vector para documento 3: [-1.14362631e-02 -9.59478132e-03  8.48004594e-03 -1.86331524e-03\n",
      "  1.11137573e-02 -4.62938519e-03  1.04465811e-02 -4.29050252e-03\n",
      "  3.37506644e-03  4.26073186e-03 -9.44148283e-03  3.98094160e-03\n",
      " -6.95270533e-03 -3.64766642e-03  6.81287562e-03  8.10831971e-03\n",
      " -1.96024007e-03 -2.09962926e-03 -4.82900039e-04  9.07886933e-05\n",
      " -1.16899619e-02 -7.28678610e-03  6.40361803e-03  1.10653033e-02\n",
      " -5.44727454e-03  4.47173463e-03 -3.57850781e-03  2.87986710e-03\n",
      " -6.27339119e-03  2.88729119e-04 -4.51030163e-03 -7.05227209e-03\n",
      "  9.47827101e-03 -8.17670953e-03 -4.58257971e-03  5.25269425e-03\n",
      " -1.07945781e-02 -7.26871379e-03 -3.98864970e-04 -7.46966526e-03\n",
      "  1.03938449e-02  1.09439734e-02  7.95302633e-03  2.81799794e-03\n",
      " -6.17991621e-03 -2.06211046e-03 -3.36275925e-03  8.56781844e-03\n",
      " -8.18799995e-03 -9.43878852e-03 -1.25066880e-02 -7.70565821e-03\n",
      " -7.99383968e-03 -3.52330669e-03 -3.90054588e-03  7.31251994e-03\n",
      "  3.46764247e-03  8.12738575e-03  4.41269425e-04  2.49594706e-03\n",
      "  6.72142440e-03  1.33724073e-02  7.79710896e-03  1.16172875e-03\n",
      " -1.52791268e-03  6.33120583e-03  8.30363762e-03  1.00918878e-02\n",
      "  4.96411230e-03  5.89040294e-03 -1.29478890e-02  3.25996918e-03\n",
      " -3.85250407e-03  3.74506763e-03  8.73648096e-03 -2.42121913e-03\n",
      " -8.24580621e-03 -5.13244048e-03 -5.89503627e-03 -2.84934975e-03\n",
      " -7.27683259e-03  6.58442453e-03 -2.51871930e-03  1.58997707e-03\n",
      " -5.83351357e-03  1.28686745e-02  4.01628698e-04 -3.34398076e-03\n",
      " -9.76476260e-03  1.90588227e-03 -1.10368971e-02 -3.89855029e-03\n",
      " -1.25427125e-02  1.07949052e-03  8.93327128e-03  9.45457537e-03\n",
      " -3.26265721e-03 -3.93591647e-04  1.28766131e-02 -7.50937918e-03]\n",
      "Vector para documento 4: [-8.86186212e-03  7.34612346e-03  1.96786318e-03  1.17670242e-02\n",
      "  2.31283440e-04 -8.74409731e-03  2.09042430e-03  1.15777291e-02\n",
      "  2.92585068e-03 -5.27394190e-03 -1.79551577e-03 -1.20534441e-02\n",
      "  7.49914953e-03  8.59998725e-03  6.49463292e-03 -7.22360518e-03\n",
      "  8.22602585e-03  3.09390156e-03  2.44433223e-03  8.38222448e-03\n",
      " -7.18784239e-03  7.38557335e-03  1.34726361e-04  1.55446744e-02\n",
      " -6.37510209e-04  9.91322752e-03 -5.27189812e-03 -4.15239530e-03\n",
      "  2.71600508e-03 -1.17944414e-02 -1.10646163e-03 -1.38604184e-04\n",
      "  3.05317901e-03  6.08667172e-03 -4.15552827e-03  7.75688968e-04\n",
      " -1.07812863e-02 -9.54295788e-03 -1.47466315e-03 -4.78512095e-03\n",
      " -3.15676443e-05  8.94336123e-03 -5.66653861e-03  6.73152460e-03\n",
      " -5.81607595e-03 -9.41736996e-03 -1.02491081e-02  7.99922552e-03\n",
      " -5.11181913e-03  8.35069548e-03 -9.85490903e-03  6.16832264e-03\n",
      " -2.67773657e-03  4.58438648e-03 -2.94852420e-03 -3.71334469e-03\n",
      " -1.16132880e-02  9.33489390e-03 -1.09711466e-02 -2.01009982e-03\n",
      "  5.99901471e-03  1.62034687e-02 -1.17654927e-04 -2.57202424e-03\n",
      "  5.82697336e-03  6.74427254e-03 -2.18375144e-03 -3.81432683e-03\n",
      " -6.36966107e-03  8.99462029e-03 -9.45629645e-03 -6.60802145e-03\n",
      " -8.72403570e-03 -3.67253274e-03  1.16512254e-02 -1.57011196e-03\n",
      "  1.06160333e-02 -1.21113248e-02  1.68779772e-03  1.07637998e-02\n",
      " -1.23863444e-02  4.15399409e-04 -1.34791192e-02  9.50619858e-03\n",
      " -3.58774676e-03  1.04985554e-02 -5.44364145e-03  2.35073362e-03\n",
      " -4.63219639e-03 -1.07644126e-02 -7.73623586e-03  5.53914171e-04\n",
      " -1.42086484e-02  4.93397019e-05 -5.00250282e-03  1.05995415e-02\n",
      " -4.70759394e-03  1.11312009e-02  5.68093173e-03 -3.50846606e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Corpus de ejemplo\n",
    "corpus = [\n",
    "    'Este es el primer documento.',\n",
    "    'Este documento es el segundo documento.',\n",
    "    'Y este es el tercer documento.',\n",
    "    'Espero que te sirva esta respuesta.'\n",
    "]\n",
    "\n",
    "# Crear etiquetas para los documentos\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(corpus)]\n",
    "\n",
    "print(tagged_data)\n",
    "\n",
    "# Definir un modelo Doc2Vec\n",
    "model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=100)\n",
    "\n",
    "# Construir el vocabulario\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "# Acceder al vocabulario\n",
    "vocab = model.wv.key_to_index\n",
    "\n",
    "# Imprimir el vocabulario\n",
    "print(vocab)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Obtener los vectores de los documentos\n",
    "document_vectors = model.dv\n",
    "\n",
    "# Mostrar los vectores de los documentos\n",
    "print(\"Vectores de los documentos:\")\n",
    "for i in range(len(corpus)):\n",
    "    print(f\"Vector para documento {i+1}: {document_vectors[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Cargar el tokenizador pre-entrenado de BERT\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Cargar el tokenizador pre-entrenado de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Cargar el modelo pre-entrenado de BERT\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = \"Replace this with your own sentence for BERT vectorization.\"\n",
    "\n",
    "# Tokenizar el texto y convertirlo en tensores\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Obtener la representaci√≥n oculta de la √∫ltima capa\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Mostrar los vectores de palabras\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin√≥nimos de 'car': {'cable_car', 'gondola', 'automobile', 'motorcar', 'car', 'auto', 'railway_car', 'elevator_car', 'railcar', 'machine', 'railroad_car'}\n",
      "Ant√≥nimos de 'good': {'ill', 'evilness', 'badness', 'bad', 'evil'}\n",
      "Definici√≥n de 'car': a motor vehicle with four wheels; usually propelled by an internal combustion engine\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Obtener sin√≥nimos de una palabra\n",
    "def obtener_sinonimos(palabra):\n",
    "    sinonimos = set()\n",
    "    for syn in wordnet.synsets(palabra):\n",
    "        for lemma in syn.lemmas():\n",
    "            sinonimos.add(lemma.name())\n",
    "    return list(sinonimos)\n",
    "\n",
    "# Obtener ant√≥nimos de una palabra\n",
    "def obtener_antonimos(palabra):\n",
    "    antonimos = set()\n",
    "    for syn in wordnet.synsets(palabra):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                antonimos.add(lemma.antonyms()[0].name())\n",
    "    return list(antonimos)\n",
    "\n",
    "# Obtener definici√≥n de una palabra\n",
    "def obtener_definicion(palabra):\n",
    "    definiciones = set()\n",
    "    for syn in wordnet.synsets(palabra):\n",
    "        definiciones.add(syn.definition())\n",
    "    return list(definiciones)\n",
    "\n",
    "# Mostrar relaciones sem√°nticas entre palabras\n",
    "def mostrar_relaciones_semanticas(palabra):\n",
    "    relaciones = []\n",
    "    for syn in wordnet.synsets(palabra):\n",
    "        relaciones.append({\n",
    "            \"Palabra\": syn.name(),\n",
    "            \"Definici√≥n\": syn.definition(),\n",
    "            \"Sin√≥nimos\": list(set(lemma.name() for lemma in syn.lemmas())),\n",
    "            \"Ant√≥nimos\": list(set(lemma.antonyms()[0].name() for lemma in syn.lemmas() if lemma.antonyms())),\n",
    "            \"Hiper√≥nimos\": list(set(syn.hypernyms())),\n",
    "            \"Hip√≥nimos\": list(set(syn.hyponyms())),\n",
    "            \"Mer√≥nimos\": list(set(syn.part_meronyms())),\n",
    "            \"Hol√≥nimos\": list(set(syn.member_holonyms())),\n",
    "        })\n",
    "    return relaciones\n",
    "\n",
    "# Ejemplo de uso\n",
    "palabra = \"car\"\n",
    "print(\"Sin√≥nimos de '{}':\".format(palabra), obtener_sinonimos(palabra))\n",
    "print(\"Ant√≥nimos de '{}':\".format(palabra), obtener_antonimos(palabra))\n",
    "print(\"Definiciones de '{}':\".format(palabra), obtener_definicion(palabra))\n",
    "print(\"\\nRelaciones sem√°nticas de '{}':\".format(palabra))\n",
    "relaciones = mostrar_relaciones_semanticas(palabra)\n",
    "for r in relaciones:\n",
    "    print(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
