{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9876bfac",
   "metadata": {},
   "source": [
    "### P1-Preprocesamiento\n",
    "\n",
    "    *Tokenizacion \n",
    "    *Eliminacion de signos de puntuacion\n",
    "    *Eliminacion de Stopwords\n",
    "    *Lematizacion\n",
    "    *Stemming\n",
    "    *Normalizar (minusculas)\n",
    "    *Bag of Words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08afb43",
   "metadata": {},
   "source": [
    "#### Importamos las librerias a utilizar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e40645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ianca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ianca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ianca\\AppData\\Local\\Temp\\ipykernel_15852\\4118414910.py\", line 8, in <module>\n",
      "    import spacy\n",
      "ModuleNotFoundError: No module named 'spacy'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2168, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1454, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1345, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1192, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1107, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 989, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 801, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ianca\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "                      ^^^^^^^^^^^^\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb265a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"La conservación del medio ambiente es vital para un futuro sostenible. Debemos adoptar prácticas ecoamigables y promover la preservación de la biodiversidad. Cada acción cuenta para proteger nuestro planeta.\"\n",
    "\n",
    "documentos = [\n",
    "    \"Para asegurar un futuro próspero, es esencial priorizar la conservación del entorno. Fomentemos hábitos respetuosos con el medio ambiente y apoyemos la biodiversidad. Cada gesto que hacemos contribuye a preservar nuestro hogar terrestre.\",\n",
    "    \"La preservación de nuestro entorno es clave para un mañana prometedor. Debemos abrazar prácticas que respeten la naturaleza y proteger la riqueza de la biodiversidad. Cada pequeña acción suma para mantener nuestro planeta saludable.\",\n",
    "    \"Para construir un porvenir próspero, es imperativo cuidar nuestro medio ambiente. Debemos adoptar conductas responsables y trabajar por la conservación de la biodiversidad. Cada esfuerzo es fundamental para proteger nuestro mundo.\",\n",
    "    \"La sostenibilidad ambiental es esencial para garantizar un futuro brillante. Es necesario fomentar hábitos ecoamigables y preservar la riqueza de la vida silvestre. Cada paso hacia la conservación cuenta en la protección de nuestro hábitat.\",\n",
    "    \"Nuestra responsabilidad con el medio ambiente define nuestro camino hacia el futuro. Promovamos prácticas amigables con la naturaleza y defendamos la diversidad biológica. Cada compromiso que asumimos es una contribución a la salvaguarda de nuestro ecosistema.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e9555",
   "metadata": {},
   "source": [
    "#### Creamos la funcion de tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da00b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    current_word = ''\n",
    "\n",
    "    for char in text:\n",
    "        if char.isalnum() or char == \"'\":\n",
    "            current_word += char\n",
    "        elif current_word:\n",
    "            tokens.append(current_word.lower())\n",
    "            current_word = ''\n",
    "    \n",
    "    if current_word:\n",
    "        tokens.append(current_word.lower())\n",
    "\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize_text(texto)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb759710",
   "metadata": {},
   "source": [
    "#### Definimos nuestras Stopwords en un arreglo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d39741",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_espanol = [\n",
    "    \"de\", \"la\", \"que\", \"el\", \"en\", \"y\", \"a\", \"los\", \"del\", \"se\", \"las\", \"por\", \"un\", \"para\", \"con\", \"no\",\n",
    "    \"una\", \"su\", \"al\", \"lo\", \"como\", \"más\", \"pero\", \"sus\", \"le\", \"ya\", \"o\", \"este\", \"sí\", \"porque\", \"esta\",\n",
    "    \"entre\", \"cuando\", \"muy\", \"sin\", \"sobre\", \"también\", \"me\", \"hasta\", \"hay\", \"donde\", \"quien\", \"desde\",\n",
    "    \"todo\", \"nos\", \"durante\", \"todos\", \"uno\", \"les\", \"ni\", \"contra\", \"otros\", \"ese\", \"eso\", \"ante\", \"ellos\",\n",
    "    \"e\", \"esto\", \"mí\", \"antes\", \"algunos\", \"qué\", \"unos\", \"yo\", \"otro\", \"otras\", \"otra\", \"él\", \"tanto\",\n",
    "    \"esa\", \"estos\", \"mucho\", \"quienes\", \"nada\", \"muchos\", \"cual\", \"poco\", \"ella\", \"estar\", \"estas\", \"algunas\",\n",
    "    \"algo\", \"nosotros\", \"mi\", \"mis\", \"tú\", \"te\", \"ti\", \"tu\", \"tus\", \"ellas\", \"nosotras\", \"vosotros\", \"vosotras\",\n",
    "    \"os\", \"mío\", \"mía\", \"míos\", \"mías\", \"tuyo\", \"tuya\", \"tuyos\", \"tuyas\", \"suyo\", \"suya\", \"suyos\", \"suyas\",\n",
    "    \"nuestro\", \"nuestra\", \"nuestros\", \"nuestras\", \"vuestro\", \"vuestra\", \"vuestros\", \"vuestras\", \"esos\", \"esas\",\n",
    "    \"estoy\", \"estás\", \"está\", \"estamos\", \"estáis\", \"están\", \"esté\", \"estés\", \"estemos\", \"estéis\", \"estén\", \"estaré\",\n",
    "    \"estarás\", \"estará\", \"estaremos\", \"estaréis\", \"estarán\", \"estaría\", \"estarías\", \"estaríamos\", \"estaríais\",\n",
    "    \"estarían\", \"estaba\", \"estabas\", \"estábamos\", \"estabais\", \"estaban\", \"estuve\", \"estuviste\", \"estuvo\", \"estuvimos\",\n",
    "    \"estuvisteis\", \"estuvieron\", \"estuviera\", \"estuvieras\", \"estuviéramos\", \"estuvierais\", \"estuvieran\", \"estuviese\",\n",
    "    \"estuvieses\", \"estuviésemos\", \"estuvieseis\", \"estuviesen\", \"estando\", \"estado\", \"estada\", \"estados\", \"estadas\",\n",
    "    \"estad\", \"he\", \"has\", \"ha\", \"hemos\", \"habéis\", \"han\", \"haya\", \"hayas\", \"hayamos\", \"hayáis\", \"hayan\", \"habré\",\n",
    "    \"habrás\", \"habrá\", \"habremos\", \"habréis\", \"habrán\", \"habría\", \"habrías\", \"habríamos\", \"habríais\", \"habrían\",\n",
    "    \"había\", \"habías\", \"habíamos\", \"habíais\", \"habían\", \"hube\", \"hubiste\", \"hubo\", \"hubimos\", \"hubisteis\", \"hubieron\",\n",
    "    \"hubiera\", \"hubieras\", \"hubiéramos\", \"hubierais\", \"hubieran\", \"hubiese\", \"hubieses\", \"hubiésemos\", \"hubieseis\",\n",
    "    \"hubiesen\", \"habiendo\", \"habido\", \"habida\", \"habidos\", \"habidas\", \"soy\", \"eres\", \"es\", \"somos\", \"sois\", \"son\",\n",
    "    \"sea\", \"seas\", \"seamos\", \"seáis\", \"sean\", \"seré\", \"serás\", \"será\", \"seremos\", \"seréis\", \"serán\", \"sería\", \"serías\",\n",
    "    \"seríamos\", \"seríais\", \"serían\", \"era\", \"eras\", \"éramos\", \"erais\", \"eran\", \"fui\", \"fuiste\", \"fue\", \"fuimos\",\n",
    "    \"fuisteis\", \"fueron\", \"fuera\", \"fueras\", \"fuéramos\", \"fuerais\", \"fueran\", \"fuese\", \"fueses\", \"fuésemos\",\n",
    "    \"fueseis\", \"fuesen\", \"sintiendo\", \"sentido\", \"sentida\", \"sentidos\", \"sentidas\", \"siente\", \"sentid\", \"tengo\",\n",
    "    \"tienes\", \"tiene\", \"tenemos\", \"tenéis\", \"tienen\", \"tenga\", \"tengas\", \"tengamos\", \"tengáis\", \"tengan\", \"tendré\",\n",
    "    \"tendrás\", \"tendrá\", \"tendremos\", \"tendréis\", \"tendrán\", \"tendría\", \"tendrías\", \"tendríamos\", \"tendríais\",\n",
    "    \"tendrían\", \"tenía\", \"tenías\", \"teníamos\", \"teníais\", \"tenían\", \"tuve\", \"tuviste\", \"tuvo\", \"tuvimos\", \"tuvisteis\",\n",
    "    \"tuvieron\", \"tuviera\", \"tuvieras\", \"tuviéramos\", \"tuvierais\", \"tuvieran\", \"tuviese\", \"tuvieses\", \"tuviésemos\",\n",
    "    \"tuvieseis\", \"tuviesen\", \"teniendo\", \"tenido\", \"tenida\", \"tenidos\", \"tenidas\", \"tened\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e27d08",
   "metadata": {},
   "source": [
    "#### Creamos la funcion de filtrado de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fa3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_stopwords(tokens, stopwords):\n",
    "    tokens_filtrados = [token for token in tokens if token.lower() not in stopwords]\n",
    "    return tokens_filtrados\n",
    "\n",
    "tokensFiltrados = filtrar_stopwords(tokens, stopwords_espanol)\n",
    "print(tokensFiltrados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382dee4",
   "metadata": {},
   "source": [
    "#### Creamos la funcion para elimiar acentos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91928f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_acentos(tokens):\n",
    "    tokensSinAcentos = [unidecode(token) for token in tokens]\n",
    "    return tokensSinAcentos\n",
    "\n",
    "tokensSinAcentos = quitar_acentos(tokensFiltrados)\n",
    "print(tokensSinAcentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39292241",
   "metadata": {},
   "source": [
    "#### Creamos a funcion que me lematiza los tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd85d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematizar(tokens):\n",
    "    lemas = []\n",
    "    for token in tokens:\n",
    "        doc = nlp(token)\n",
    "        for token in doc:\n",
    "            lemas.append(token.lemma_)\n",
    "    return lemas\n",
    "\n",
    "lemas = lematizar(tokensSinAcentos)\n",
    "print(lemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613b902",
   "metadata": {},
   "source": [
    "#### Creamos la funcion que nos hace el Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def stemming(tokens):\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return stems\n",
    "\n",
    "raices = stemming(tokensSinAcentos)\n",
    "print(raices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67989a",
   "metadata": {},
   "source": [
    "#### Creamos la funcion de las bag of words para vectorizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc72b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bag_of_words(documents):\n",
    "    # Creamos un conjunto para almacenar todas las palabras únicas en los documentos\n",
    "    vocabulary = set()\n",
    "    for document in documents:\n",
    "        vocabulary.update(document)\n",
    "\n",
    "    # Creamos un diccionario para almacenar las frecuencias de las palabras en cada documento\n",
    "    bow = []\n",
    "    for document in documents:\n",
    "        doc_bow = {}\n",
    "        for word in vocabulary:\n",
    "            doc_bow[word] = document.count(word)\n",
    "        bow.append(doc_bow)\n",
    "\n",
    "    return bow, list(vocabulary)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "\n",
    "docNormalizado = []\n",
    "for doc in documentos:\n",
    "    docNormalizado.append(quitar_acentos(filtrar_stopwords(tokenize_text(doc), stopwords_espanol)))\n",
    "\n",
    "bow_representation, vocabulary = bag_of_words(docNormalizado)\n",
    "print(\"Bag of Words representation:\")\n",
    "for doc_bow in bow_representation:\n",
    "    print(doc_bow)\n",
    "print(\"\\nVocabulary:\", vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91cc9c3-f431-4355-83fd-8405ef5e9a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
