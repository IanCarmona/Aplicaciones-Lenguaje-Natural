{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9876bfac",
   "metadata": {},
   "source": [
    "### P1-Preprocesamiento\n",
    "\n",
    "    *Tokenizacion \n",
    "    *Eliminacion de signos de puntuacion\n",
    "    *Eliminacion de Stopwords\n",
    "    *Lematizacion\n",
    "    *Stemming\n",
    "    *Normalizar (minusculas)\n",
    "    *Bag of Words\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08afb43",
   "metadata": {},
   "source": [
    "#### Importamos las librerias a utilizar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e40645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ianca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ianca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb265a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = pd.read_csv(\"./datasets/sentiment_analysis_dataset.csv\")\n",
    "texto = texto[\"text\"]\n",
    "\n",
    "doc_ejemplos = [\n",
    "    'este es el primer documento',\n",
    "    'este documento es el segundo documento',\n",
    "    'y este es el tercer documento',\n",
    "    'es este el primer documento'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e9555",
   "metadata": {},
   "source": [
    "#### Creamos la funcion de tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3da00b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['este', 'es', 'el', 'primer', 'documento'], ['este', 'documento', 'es', 'el', 'segundo', 'documento'], ['y', 'este', 'es', 'el', 'tercer', 'documento'], ['es', 'este', 'el', 'primer', 'documento']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    current_word = ''\n",
    "\n",
    "    for char in text:\n",
    "        if char.isalnum() or char == \"'\":\n",
    "            current_word += char\n",
    "        elif current_word:\n",
    "            tokens.append(current_word.lower())\n",
    "            current_word = ''\n",
    "    \n",
    "    if current_word:\n",
    "        tokens.append(current_word.lower())\n",
    "\n",
    "    return tokens\n",
    "\n",
    "allTokens = []\n",
    "tokenEjm = []\n",
    "\n",
    "for row in texto:\n",
    "    tokens = tokenize_text(row)    \n",
    "    allTokens.append(tokens)\n",
    "    \n",
    "for row in doc_ejemplos:\n",
    "    tokens2 = tokenize_text(row)    \n",
    "    tokenEjm.append(tokens2)\n",
    "    \n",
    "print(tokenEjm) #Funciona\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb759710",
   "metadata": {},
   "source": [
    "#### Definimos nuestras Stopwords en un arreglo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d39741",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_espanol = [\n",
    "    \"de\", \"la\", \"que\", \"el\", \"en\", \"y\", \"a\", \"los\", \"del\", \"se\", \"las\", \"por\", \"un\", \"para\", \"con\", \"no\",\n",
    "    \"una\", \"su\", \"al\", \"lo\", \"como\", \"más\", \"pero\", \"sus\", \"le\", \"ya\", \"o\", \"este\", \"sí\", \"porque\", \"esta\",\n",
    "    \"entre\", \"cuando\", \"muy\", \"sin\", \"sobre\", \"también\", \"me\", \"hasta\", \"hay\", \"donde\", \"quien\", \"desde\",\n",
    "    \"todo\", \"nos\", \"durante\", \"todos\", \"uno\", \"les\", \"ni\", \"contra\", \"otros\", \"ese\", \"eso\", \"ante\", \"ellos\",\n",
    "    \"e\", \"esto\", \"mí\", \"antes\", \"algunos\", \"qué\", \"unos\", \"yo\", \"otro\", \"otras\", \"otra\", \"él\", \"tanto\",\n",
    "    \"esa\", \"estos\", \"mucho\", \"quienes\", \"nada\", \"muchos\", \"cual\", \"poco\", \"ella\", \"estar\", \"estas\", \"algunas\",\n",
    "    \"algo\", \"nosotros\", \"mi\", \"mis\", \"tú\", \"te\", \"ti\", \"tu\", \"tus\", \"ellas\", \"nosotras\", \"vosotros\", \"vosotras\",\n",
    "    \"os\", \"mío\", \"mía\", \"míos\", \"mías\", \"tuyo\", \"tuya\", \"tuyos\", \"tuyas\", \"suyo\", \"suya\", \"suyos\", \"suyas\",\n",
    "    \"nuestro\", \"nuestra\", \"nuestros\", \"nuestras\", \"vuestro\", \"vuestra\", \"vuestros\", \"vuestras\", \"esos\", \"esas\",\n",
    "    \"estoy\", \"estás\", \"está\", \"estamos\", \"estáis\", \"están\", \"esté\", \"estés\", \"estemos\", \"estéis\", \"estén\", \"estaré\",\n",
    "    \"estarás\", \"estará\", \"estaremos\", \"estaréis\", \"estarán\", \"estaría\", \"estarías\", \"estaríamos\", \"estaríais\",\n",
    "    \"estarían\", \"estaba\", \"estabas\", \"estábamos\", \"estabais\", \"estaban\", \"estuve\", \"estuviste\", \"estuvo\", \"estuvimos\",\n",
    "    \"estuvisteis\", \"estuvieron\", \"estuviera\", \"estuvieras\", \"estuviéramos\", \"estuvierais\", \"estuvieran\", \"estuviese\",\n",
    "    \"estuvieses\", \"estuviésemos\", \"estuvieseis\", \"estuviesen\", \"estando\", \"estado\", \"estada\", \"estados\", \"estadas\",\n",
    "    \"estad\", \"he\", \"has\", \"ha\", \"hemos\", \"habéis\", \"han\", \"haya\", \"hayas\", \"hayamos\", \"hayáis\", \"hayan\", \"habré\",\n",
    "    \"habrás\", \"habrá\", \"habremos\", \"habréis\", \"habrán\", \"habría\", \"habrías\", \"habríamos\", \"habríais\", \"habrían\",\n",
    "    \"había\", \"habías\", \"habíamos\", \"habíais\", \"habían\", \"hube\", \"hubiste\", \"hubo\", \"hubimos\", \"hubisteis\", \"hubieron\",\n",
    "    \"hubiera\", \"hubieras\", \"hubiéramos\", \"hubierais\", \"hubieran\", \"hubiese\", \"hubieses\", \"hubiésemos\", \"hubieseis\",\n",
    "    \"hubiesen\", \"habiendo\", \"habido\", \"habida\", \"habidos\", \"habidas\", \"soy\", \"eres\", \"es\", \"somos\", \"sois\", \"son\",\n",
    "    \"sea\", \"seas\", \"seamos\", \"seáis\", \"sean\", \"seré\", \"serás\", \"será\", \"seremos\", \"seréis\", \"serán\", \"sería\", \"serías\",\n",
    "    \"seríamos\", \"seríais\", \"serían\", \"era\", \"eras\", \"éramos\", \"erais\", \"eran\", \"fui\", \"fuiste\", \"fue\", \"fuimos\",\n",
    "    \"fuisteis\", \"fueron\", \"fuera\", \"fueras\", \"fuéramos\", \"fuerais\", \"fueran\", \"fuese\", \"fueses\", \"fuésemos\",\n",
    "    \"fueseis\", \"fuesen\", \"sintiendo\", \"sentido\", \"sentida\", \"sentidos\", \"sentidas\", \"siente\", \"sentid\", \"tengo\",\n",
    "    \"tienes\", \"tiene\", \"tenemos\", \"tenéis\", \"tienen\", \"tenga\", \"tengas\", \"tengamos\", \"tengáis\", \"tengan\", \"tendré\",\n",
    "    \"tendrás\", \"tendrá\", \"tendremos\", \"tendréis\", \"tendrán\", \"tendría\", \"tendrías\", \"tendríamos\", \"tendríais\",\n",
    "    \"tendrían\", \"tenía\", \"tenías\", \"teníamos\", \"teníais\", \"tenían\", \"tuve\", \"tuviste\", \"tuvo\", \"tuvimos\", \"tuvisteis\",\n",
    "    \"tuvieron\", \"tuviera\", \"tuvieras\", \"tuviéramos\", \"tuvierais\", \"tuvieran\", \"tuviese\", \"tuvieses\", \"tuviésemos\",\n",
    "    \"tuvieseis\", \"tuviesen\", \"teniendo\", \"tenido\", \"tenida\", \"tenidos\", \"tenidas\", \"tened\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e27d08",
   "metadata": {},
   "source": [
    "#### Creamos la funcion de filtrado de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "775fa3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['primer', 'documento'], ['documento', 'segundo', 'documento'], ['tercer', 'documento'], ['primer', 'documento']]\n"
     ]
    }
   ],
   "source": [
    "def filtrar_stopwords(tokens, stopwords):\n",
    "    tokens_filtrados = [token for token in tokens if len(token) > 3 and token.lower() not in stopwords]\n",
    "    return tokens_filtrados\n",
    "\n",
    "allTokensFiltrados = []\n",
    "tokenStop = []\n",
    "\n",
    "for token in allTokens:\n",
    "    tokensFiltrados = filtrar_stopwords(token, stopwords_espanol)\n",
    "    allTokensFiltrados.append(tokensFiltrados)\n",
    "\n",
    "#print(allTokensFiltrados) #Funciona\n",
    "for row in tokenEjm:\n",
    "    tokens2 = filtrar_stopwords(row, stopwords_espanol)    \n",
    "    tokenStop.append(tokens2)\n",
    "    \n",
    "print(tokenStop) #Funciona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382dee4",
   "metadata": {},
   "source": [
    "#### Creamos la funcion para elimiar acentos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91928f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['primer', 'documento'], ['documento', 'segundo', 'documento'], ['tercer', 'documento'], ['primer', 'documento']]\n"
     ]
    }
   ],
   "source": [
    "def quitar_acentos(tokens):\n",
    "    tokensSinAcentos = [unidecode(token) for token in tokens]\n",
    "    return tokensSinAcentos\n",
    "\n",
    "\n",
    "allTokensSinAcentos = []\n",
    "tokenSin = []\n",
    "for token in allTokensFiltrados:\n",
    "    tokensSinAcentos = quitar_acentos(token)\n",
    "    allTokensSinAcentos.append(tokensSinAcentos)\n",
    "    \n",
    "for token in tokenStop:\n",
    "    tokensSinAcentos = quitar_acentos(token)\n",
    "    tokenSin.append(tokensSinAcentos)\n",
    "    \n",
    "#print(allTokensSinAcentos) #Funciona\n",
    "print(tokenSin) #Funciona\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39292241",
   "metadata": {},
   "source": [
    "#### Creamos a funcion que me lematiza los tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd85d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['primero', 'documento'], ['documento', 'segundo', 'documento'], ['tercer', 'documento'], ['primero', 'documento']]\n"
     ]
    }
   ],
   "source": [
    "def lematizar(tokens):\n",
    "    lemas = []\n",
    "    for token in tokens:\n",
    "        doc = nlp(token)\n",
    "        for token in doc:\n",
    "            lemas.append(token.lemma_)\n",
    "    return lemas\n",
    "\n",
    "allTokensLematizados = []\n",
    "tokenlem = []\n",
    "\n",
    "for token in allTokensSinAcentos:\n",
    "    lemas = lematizar(token)\n",
    "    allTokensLematizados.append(lemas)\n",
    "    \n",
    "for token in tokenSin:\n",
    "    lemas = lematizar(token)\n",
    "    tokenlem.append(lemas)\n",
    "\n",
    "#print(allTokensLematizados) #Funciona\n",
    "print(tokenlem) #Funciona\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613b902",
   "metadata": {},
   "source": [
    "#### Creamos la funcion que nos hace el Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d9c514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['prim', 'document'], ['document', 'segund', 'document'], ['terc', 'document'], ['prim', 'document']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def stemming(tokens):\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return stems\n",
    "\n",
    "allTokensStemming = []\n",
    "tokenStem = []\n",
    "\n",
    "for token in allTokensSinAcentos:\n",
    "    raices = stemming(token)\n",
    "    allTokensStemming.append(raices)\n",
    "    \n",
    "for token in tokenSin:\n",
    "    raices = stemming(token)\n",
    "    tokenStem.append(raices)\n",
    "    \n",
    "#print(allTokensStemming) #Funciona\n",
    "print(tokenStem) #Funciona\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67989a",
   "metadata": {},
   "source": [
    "#### Creamos la funcion de las bag of words para vectorizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bebc72b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words representation:\n",
      "{'documento': 1, 'primero': 1, 'segundo': 0, 'tercer': 0}\n",
      "{'documento': 2, 'primero': 0, 'segundo': 1, 'tercer': 0}\n",
      "{'documento': 1, 'primero': 0, 'segundo': 0, 'tercer': 1}\n",
      "{'documento': 1, 'primero': 1, 'segundo': 0, 'tercer': 0}\n",
      "\n",
      "Vocabulary: ['documento', 'primero', 'segundo', 'tercer']\n",
      "Se ha guardado la frecuencia de palabras en el archivo: BOW.txt\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(documents):\n",
    "    # Creamos un conjunto para almacenar todas las palabras únicas en los documentos\n",
    "    vocabulary = set()\n",
    "    for document in documents:\n",
    "        vocabulary.update(document)\n",
    "\n",
    "    # Creamos un diccionario para almacenar las frecuencias de las palabras en cada documento\n",
    "    bow = []\n",
    "    for document in documents:\n",
    "        doc_bow = {}\n",
    "        for word in vocabulary:\n",
    "            doc_bow[word] = document.count(word)\n",
    "        bow.append(doc_bow)\n",
    "\n",
    "    return bow, list(vocabulary)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "\n",
    "bow_representation, vocabulary = bag_of_words(allTokensLematizados)\n",
    "\n",
    "nombre_archivo = \"resultados-bag-of-words.txt\"\n",
    "\n",
    "with open(nombre_archivo, \"w\") as archivo:\n",
    "    archivo.write(\"Bag of Words representation:\\n\")\n",
    "    for doc_bow in bow_representation:\n",
    "        archivo.write(str(doc_bow) + \"\\n\")\n",
    "    archivo.write(\"\\nVocabulary: \" + str(vocabulary))\n",
    "\n",
    "bow_representation2, vocabulary2 = bag_of_words(tokenlem)\n",
    "\n",
    "print(\"Bag of Words representation:\")\n",
    "for doc_bow in bow_representation2:\n",
    "    print(doc_bow)\n",
    "print(\"\\nVocabulary:\", vocabulary2)\n",
    "\n",
    "\n",
    "# Creamos un diccionario para almacenar la frecuencia de cada palabra en todo el corpus\n",
    "frecuencia_palabras = {}\n",
    "\n",
    "# Recorremos todos los documentos en el primer conjunto de datos\n",
    "for doc_bow in bow_representation:\n",
    "    for palabra, frecuencia in doc_bow.items():\n",
    "        if palabra in frecuencia_palabras:\n",
    "            frecuencia_palabras[palabra] += frecuencia\n",
    "        else:\n",
    "            frecuencia_palabras[palabra] = frecuencia\n",
    "\n",
    "# Recorremos todos los documentos en el segundo conjunto de datos\n",
    "for doc_bow in bow_representation2:\n",
    "    for palabra, frecuencia in doc_bow.items():\n",
    "        if palabra in frecuencia_palabras:\n",
    "            frecuencia_palabras[palabra] += frecuencia\n",
    "        else:\n",
    "            frecuencia_palabras[palabra] = frecuencia\n",
    "\n",
    "nombre_archivo_frecuencia = \"BOW.txt\"\n",
    "\n",
    "with open(nombre_archivo_frecuencia, \"w\") as archivo_frecuencia:\n",
    "    archivo_frecuencia.write(\"Frecuencia de palabras en todo el corpus:\\n\")\n",
    "    for palabra, frecuencia in frecuencia_palabras.items():\n",
    "        archivo_frecuencia.write(f\"{palabra}: {frecuencia}\\n\")\n",
    "\n",
    "print(f\"Se ha guardado la frecuencia de palabras en el archivo: {nombre_archivo_frecuencia}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf09c5",
   "metadata": {},
   "source": [
    "#### Preprocesamiento final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6c7322a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                          terminar bien abrumar despues \n",
      "1                                          sentir abrumar\n",
      "2       sentir abrumar cantidad cosa querer dibujar ju...\n",
      "3       salvador unicar persona abrumar versión nadiec...\n",
      "4                           denmir helado ar full abrumar\n",
      "                              ...                        \n",
      "2585    poder vivir miedo manejen borracho dejar usar ...\n",
      "2586                           vida constante miedo exito\n",
      "2587    esquizofrenia mente dividido miedo realidad pa...\n",
      "2588               miedo como desaparez mundo temer mundo\n",
      "2589               saltar apocir pile agua miedo relajado\n",
      "Name: text, Length: 2590, dtype: object\n"
     ]
    }
   ],
   "source": [
    "archivo_csv_original = './datasets/sentiment_analysis_dataset.csv'\n",
    "archivo_csv_salida = './datasets/sentiment_analysis_dataset_modificado.csv'\n",
    "\n",
    "df_original = pd.read_csv(archivo_csv_original)\n",
    "\n",
    "documentos_texto = [\" \".join(tokens) for tokens in allTokensLematizados]\n",
    "\n",
    "df_original['text'] = documentos_texto\n",
    "df_original.to_csv(archivo_csv_salida, index=False)\n",
    "\n",
    "print(df_original['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
