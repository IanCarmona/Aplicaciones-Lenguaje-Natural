{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9876bfac",
   "metadata": {},
   "source": [
    "### P1-Preprocesamiento\n",
    "\n",
    "    *Tokenizacion \n",
    "    *Eliminacion de signos de puntuacion\n",
    "    *Eliminacion de Stopwords\n",
    "    *Lematizacion\n",
    "    *Stemming\n",
    "    *Normalizar (minusculas)\n",
    "    *Bag of Words\n",
    "    *TF-IDF\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08afb43",
   "metadata": {},
   "source": [
    "#### Importamos las librerias a utilizar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30e40645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ianca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ianca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb265a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = pd.read_csv(\"./datasets/sentiment_analysis_dataset.csv\")\n",
    "texto = texto[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e9555",
   "metadata": {},
   "source": [
    "#### Creamos la funcion de tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3da00b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    current_word = ''\n",
    "\n",
    "    for char in text:\n",
    "        if char.isalnum() or char == \"'\":\n",
    "            current_word += char\n",
    "        elif current_word:\n",
    "            tokens.append(current_word.lower())\n",
    "            current_word = ''\n",
    "    \n",
    "    if current_word:\n",
    "        tokens.append(current_word.lower())\n",
    "\n",
    "    return tokens\n",
    "\n",
    "allTokens = []\n",
    "for row in texto:\n",
    "    tokens = tokenize_text(row)\n",
    "    allTokens.append(tokens)\n",
    "    \n",
    "# print(allTokens) #Funciona\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb759710",
   "metadata": {},
   "source": [
    "#### Definimos nuestras Stopwords en un arreglo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1d39741",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_espanol = [\n",
    "    \"de\", \"la\", \"que\", \"el\", \"en\", \"y\", \"a\", \"los\", \"del\", \"se\", \"las\", \"por\", \"un\", \"para\", \"con\", \"no\",\n",
    "    \"una\", \"su\", \"al\", \"lo\", \"como\", \"más\", \"pero\", \"sus\", \"le\", \"ya\", \"o\", \"este\", \"sí\", \"porque\", \"esta\",\n",
    "    \"entre\", \"cuando\", \"muy\", \"sin\", \"sobre\", \"también\", \"me\", \"hasta\", \"hay\", \"donde\", \"quien\", \"desde\",\n",
    "    \"todo\", \"nos\", \"durante\", \"todos\", \"uno\", \"les\", \"ni\", \"contra\", \"otros\", \"ese\", \"eso\", \"ante\", \"ellos\",\n",
    "    \"e\", \"esto\", \"mí\", \"antes\", \"algunos\", \"qué\", \"unos\", \"yo\", \"otro\", \"otras\", \"otra\", \"él\", \"tanto\",\n",
    "    \"esa\", \"estos\", \"mucho\", \"quienes\", \"nada\", \"muchos\", \"cual\", \"poco\", \"ella\", \"estar\", \"estas\", \"algunas\",\n",
    "    \"algo\", \"nosotros\", \"mi\", \"mis\", \"tú\", \"te\", \"ti\", \"tu\", \"tus\", \"ellas\", \"nosotras\", \"vosotros\", \"vosotras\",\n",
    "    \"os\", \"mío\", \"mía\", \"míos\", \"mías\", \"tuyo\", \"tuya\", \"tuyos\", \"tuyas\", \"suyo\", \"suya\", \"suyos\", \"suyas\",\n",
    "    \"nuestro\", \"nuestra\", \"nuestros\", \"nuestras\", \"vuestro\", \"vuestra\", \"vuestros\", \"vuestras\", \"esos\", \"esas\",\n",
    "    \"estoy\", \"estás\", \"está\", \"estamos\", \"estáis\", \"están\", \"esté\", \"estés\", \"estemos\", \"estéis\", \"estén\", \"estaré\",\n",
    "    \"estarás\", \"estará\", \"estaremos\", \"estaréis\", \"estarán\", \"estaría\", \"estarías\", \"estaríamos\", \"estaríais\",\n",
    "    \"estarían\", \"estaba\", \"estabas\", \"estábamos\", \"estabais\", \"estaban\", \"estuve\", \"estuviste\", \"estuvo\", \"estuvimos\",\n",
    "    \"estuvisteis\", \"estuvieron\", \"estuviera\", \"estuvieras\", \"estuviéramos\", \"estuvierais\", \"estuvieran\", \"estuviese\",\n",
    "    \"estuvieses\", \"estuviésemos\", \"estuvieseis\", \"estuviesen\", \"estando\", \"estado\", \"estada\", \"estados\", \"estadas\",\n",
    "    \"estad\", \"he\", \"has\", \"ha\", \"hemos\", \"habéis\", \"han\", \"haya\", \"hayas\", \"hayamos\", \"hayáis\", \"hayan\", \"habré\",\n",
    "    \"habrás\", \"habrá\", \"habremos\", \"habréis\", \"habrán\", \"habría\", \"habrías\", \"habríamos\", \"habríais\", \"habrían\",\n",
    "    \"había\", \"habías\", \"habíamos\", \"habíais\", \"habían\", \"hube\", \"hubiste\", \"hubo\", \"hubimos\", \"hubisteis\", \"hubieron\",\n",
    "    \"hubiera\", \"hubieras\", \"hubiéramos\", \"hubierais\", \"hubieran\", \"hubiese\", \"hubieses\", \"hubiésemos\", \"hubieseis\",\n",
    "    \"hubiesen\", \"habiendo\", \"habido\", \"habida\", \"habidos\", \"habidas\", \"soy\", \"eres\", \"es\", \"somos\", \"sois\", \"son\",\n",
    "    \"sea\", \"seas\", \"seamos\", \"seáis\", \"sean\", \"seré\", \"serás\", \"será\", \"seremos\", \"seréis\", \"serán\", \"sería\", \"serías\",\n",
    "    \"seríamos\", \"seríais\", \"serían\", \"era\", \"eras\", \"éramos\", \"erais\", \"eran\", \"fui\", \"fuiste\", \"fue\", \"fuimos\",\n",
    "    \"fuisteis\", \"fueron\", \"fuera\", \"fueras\", \"fuéramos\", \"fuerais\", \"fueran\", \"fuese\", \"fueses\", \"fuésemos\",\n",
    "    \"fueseis\", \"fuesen\", \"sintiendo\", \"sentido\", \"sentida\", \"sentidos\", \"sentidas\", \"siente\", \"sentid\", \"tengo\",\n",
    "    \"tienes\", \"tiene\", \"tenemos\", \"tenéis\", \"tienen\", \"tenga\", \"tengas\", \"tengamos\", \"tengáis\", \"tengan\", \"tendré\",\n",
    "    \"tendrás\", \"tendrá\", \"tendremos\", \"tendréis\", \"tendrán\", \"tendría\", \"tendrías\", \"tendríamos\", \"tendríais\",\n",
    "    \"tendrían\", \"tenía\", \"tenías\", \"teníamos\", \"teníais\", \"tenían\", \"tuve\", \"tuviste\", \"tuvo\", \"tuvimos\", \"tuvisteis\",\n",
    "    \"tuvieron\", \"tuviera\", \"tuvieras\", \"tuviéramos\", \"tuvierais\", \"tuvieran\", \"tuviese\", \"tuvieses\", \"tuviésemos\",\n",
    "    \"tuvieseis\", \"tuviesen\", \"teniendo\", \"tenido\", \"tenida\", \"tenidos\", \"tenidas\", \"tened\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e27d08",
   "metadata": {},
   "source": [
    "#### Creamos la funcion de filtrado de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "775fa3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_stopwords(tokens, stopwords):\n",
    "    tokens_filtrados = [token for token in tokens if token.lower() not in stopwords]\n",
    "    return tokens_filtrados\n",
    "\n",
    "allTokensFiltrados = []\n",
    "\n",
    "for token in allTokens:\n",
    "    tokensFiltrados = filtrar_stopwords(token, stopwords_espanol)\n",
    "    allTokensFiltrados.append(tokensFiltrados)\n",
    "\n",
    "#print(allTokensFiltrados) #Funciona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382dee4",
   "metadata": {},
   "source": [
    "#### Creamos la funcion para elimiar acentos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91928f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_acentos(tokens):\n",
    "    tokensSinAcentos = [unidecode(token) for token in tokens]\n",
    "    return tokensSinAcentos\n",
    "\n",
    "\n",
    "allTokensSinAcentos = []\n",
    "for token in allTokensFiltrados:\n",
    "    tokensSinAcentos = quitar_acentos(token)\n",
    "    allTokensSinAcentos.append(tokensSinAcentos)\n",
    "    \n",
    "#print(allTokensSinAcentos) #Funciona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39292241",
   "metadata": {},
   "source": [
    "#### Creamos a funcion que me lematiza los tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbd85d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematizar(tokens):\n",
    "    lemas = []\n",
    "    for token in tokens:\n",
    "        doc = nlp(token)\n",
    "        for token in doc:\n",
    "            lemas.append(token.lemma_)\n",
    "    return lemas\n",
    "\n",
    "allTokensLematizados = []\n",
    "for token in allTokensSinAcentos:\n",
    "    lemas = lematizar(token)\n",
    "    allTokensLematizados.append(lemas)\n",
    "\n",
    "#print(allTokensLematizados) #Funciona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613b902",
   "metadata": {},
   "source": [
    "#### Creamos la funcion que nos hace el Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d9c514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def stemming(tokens):\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return stems\n",
    "\n",
    "allTokensStemming = []\n",
    "for token in allTokensSinAcentos:\n",
    "    raices = stemming(token)\n",
    "    allTokensStemming.append(raices)\n",
    "    \n",
    "#print(allTokensStemming) #Funciona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67989a",
   "metadata": {},
   "source": [
    "#### Creamos la funcion de las bag of words para vectorizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bebc72b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bag_of_words(documents):\n",
    "    # Creamos un conjunto para almacenar todas las palabras únicas en los documentos\n",
    "    vocabulary = set()\n",
    "    for document in documents:\n",
    "        vocabulary.update(document)\n",
    "\n",
    "    # Creamos un diccionario para almacenar las frecuencias de las palabras en cada documento\n",
    "    bow = []\n",
    "    for document in documents:\n",
    "        doc_bow = {}\n",
    "        for word in vocabulary:\n",
    "            doc_bow[word] = document.count(word)\n",
    "        bow.append(doc_bow)\n",
    "\n",
    "    return bow, list(vocabulary)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "\n",
    "bow_representation, vocabulary = bag_of_words(allTokensSinAcentos)\n",
    "# print(\"Bag of Words representation:\")\n",
    "# for doc_bow in bow_representation:\n",
    "#     print(doc_bow)\n",
    "# print(\"\\nVocabulary:\", vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dbb2a9",
   "metadata": {},
   "source": [
    "#### Funcion TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a3dcf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   documento        el        es      este    primer   segundo    tercer\n",
      "0   0.398961  0.398961  0.398961  0.398961  0.602761  0.000000  0.000000\n",
      "1   0.612215  0.306107  0.306107  0.306107  0.000000  0.586591  0.000000\n",
      "2   0.361028  0.361028  0.361028  0.361028  0.000000  0.000000  0.691835\n",
      "3   0.398961  0.398961  0.398961  0.398961  0.602761  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "def calcular_tfidf(documentos_tokens):\n",
    "    # Convertir la lista de listas de tokens en documentos de texto nuevamente\n",
    "    documentos_texto = [\" \".join(tokens) for tokens in documentos_tokens]\n",
    "    \n",
    "    # Inicializar el vectorizador TF-IDF\n",
    "    vectorizador_tfidf = TfidfVectorizer()\n",
    "\n",
    "    # Calcular TF-IDF\n",
    "    matriz_tfidf = vectorizador_tfidf.fit_transform(documentos_texto)\n",
    "\n",
    "    # Obtener los nombres de las características (palabras)\n",
    "    palabras = vectorizador_tfidf.get_feature_names_out()\n",
    "\n",
    "    # Crear un DataFrame para visualizar los resultados\n",
    "    df_tfidf = pd.DataFrame(matriz_tfidf.toarray(), columns=palabras)\n",
    "\n",
    "    return df_tfidf, documentos_texto\n",
    "\n",
    "documentos_tokens = [\n",
    "    [\"este\", \"es\", \"el\", \"primer\", \"documento\"],\n",
    "    [\"este\", \"documento\", \"es\", \"el\", \"segundo\", \"documento\"],\n",
    "    [\"y\", \"este\", \"es\", \"el\", \"tercer\", \"documento\"],\n",
    "    [\"es\", \"este\", \"el\", \"primer\", \"documento\"]\n",
    "]\n",
    "\n",
    "# Calcular TF-IDF\n",
    "resultado_tfidf, doc = calcular_tfidf(allTokensSinAcentos)\n",
    "resultado_prueba_tfidf, d = calcular_tfidf(documentos_tokens)\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(resultado_prueba_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6c7322a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                       termine bien abrumado despues hoy\n",
      "1                                         siento abrumado\n",
      "2       siento abrumado cantidad cosas quiero dibujar ...\n",
      "3       salvador unica persona abrumado versiones nadi...\n",
      "4                         denme helado ando full abrumado\n",
      "                              ...                        \n",
      "2585    podemos vivir miedo manejen borrachos dejen us...\n",
      "2586                           vida constante miedo exito\n",
      "2587    esquizofrenia mente dividida miedo realidades ...\n",
      "2588       miedo da ver como desapareces mundo temo mundo\n",
      "2589       saltando apoco ala pile agua xd miedo relajado\n",
      "Name: text, Length: 2590, dtype: object\n"
     ]
    }
   ],
   "source": [
    "archivo_csv_original = './datasets/sentiment_analysis_dataset.csv'\n",
    "archivo_csv_salida = './datasets/sentiment_analysis_dataset_modificado.csv'\n",
    "\n",
    "df_original = pd.read_csv(archivo_csv_original)\n",
    "\n",
    "df_original['text'] = doc\n",
    "df_original.to_csv(archivo_csv_salida, index=False)\n",
    "\n",
    "print(df_original['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
